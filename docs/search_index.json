[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 9 1.1 K-means 1.2 K-medoids", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 9 In week 9, we studied about partitioning cluster analysis and more specifically, \\(K\\)-means clustering and \\(K\\)-medoids clustering. 1.1 K-means \\(K\\)-means is implemented in R using the function kmeans in the stats package. As explained in Week 9 lecture note, the function takes the following arguments: x: numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns). centers: either the number of clusters, \\(K\\), or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres. iter.max: the maximum number of iterations allowed. nstart: if centers is a number, how many random sets should be chosen. The last two arguments are required as \\(K\\)-means is implemented using an iterative algorithm and thus may take a few iterations to converge to local optimum, and it is recommended to run the algorithm multiple times to find the best initialisation that leads to a relatively good local optimum. kmeans return a list of following components (it is important to understand what each component means): kmeans returns an object of class \"kmeans\" which has a print and a fitted method. It is a list with at least the following components: cluster: A vector of integers (from 1:\\(K\\)) with length equal to the number of observations indicating the cluster to which each point is allocated. centers: A matrix of cluster centres with each row representing a cluster centre. totss: The total sum of squares. withinss: Vector of within-cluster sum of squares, one per cluster. tot.withinss: Total within-cluster sum of squares, i.e. sum(withinss). betweenss: The between-cluster sum of squares, i.e. totss-tot.withinss. size: The number of points in each cluster. iter: The number of (outer) iterations. 1.2 K-medoids \\(K\\)-medoids is implemented by using the function pam in the cluster package. Its usage and arguments are similar to kmeans, except an additional argument metric to allow for the flexibility in using non-Euclidean distances. x: data matrix or data frame, or dissimilarity matrix. k: the number of clusters. metric: the metric to be used for calculating dissimilarities between observations. The currently available options are \"euclidean\" and \"manhattan\". medoids: length-k vector of integer indices specifying initial medoids; NULL (default). nstart: the number of random \"starts\"; used only when medoids = \"random\" The output returned by pam includes: medoids: The medoids or representative objects of the clusters. id.med: A vector of integers with length \\(K\\) which specifies the indices giving the medoid observation numbers. clustering: A vector of integers (from 1:\\(K\\)) with length equal to the number of observations indicating the cluster to which each point is allocated. clusinfo: A matrix where each row gives numerical information for one cluster, such as cluster size, maximum dissimilarity within the cluster, average dissimilarity. silinfo: A list of silhouette width information. "],["exercise-1-usarrests.html", "2 Exercise 1: USArrests", " 2 Exercise 1: USArrests In this exercise, we will continue studying the USArrest data and apply \\(K\\)-means for clustering. QUESTION: Perform \\(K\\)-means clustering with your chosen value of \\(K\\). (\\(K\\) does not have to be optimal at this stage.) Solution Suppose we would like to group the data into 3 clusters. The corresponding R code is: set.seed(1) USArrests &lt;- data.frame(scale(USArrests)) usarr.kcl &lt;- kmeans(USArrests, centers=3, nstart=100) Visualise the \\(K\\)-means result and compare with HAC result obtained in Lab 8. Solution plot(USArrests, col = usarr.kcl$cluster) The number of observations in each cluster seems to be more balanced in the \\(K\\)-means case compared with the HAC case. In addition, as \\(K\\)-means assign observations to their nearest centre, there seems to be slightly less overlapping of clusters. Get the cluster means and append the cluster assignment to a new data frame. Hint Use $cluster to extract cluster assignment and aggregate function to calculate cluster means. The latter function has been studied in Lab 3 (Section 3.1). Solution # get cluster means aggregate(USArrests,by=list(usarr.kcl$cluster),FUN=mean) ## Group.1 Murder Assault UrbanPop Rape ## 1 1 -0.9615407 -1.1066010 -0.9301069 -0.9667633 ## 2 2 -0.4469795 -0.3465138 0.4788049 -0.2571398 ## 3 3 1.0049340 1.0138274 0.1975853 0.8469650 # append cluster assignment USArrests_cl &lt;- data.frame(USArrests, usarr.kcl$cluster) Plot the cluster results (saved in the new data frame) using the clusplot function within the cluster library. Solution library(cluster) clusplot(USArrests_cl, usarr.kcl$cluster, color=TRUE, shade=TRUE, labels=2, lines=0) The clusplot function uses principal component analysis (PCA) to draw the data. Thus, Component 1 and Component 2 refer to the first and second PCs, and These two components explain xx% of the point variability lists the cumulative proportion of variation explained by the first two PCs. (optional) Now perform kmeans with a different value of \\(K\\). By taking advantage of the cluster.stats function, compare the similarity between two cluster results. This refers to the supplementary material in Week 9 lecture note. Solution We have already said that choosing \\(K\\) (i.e. the numbers of clusters) can sometimes look arbitrary. In that case we may be interested to compare the similarity between two cluster solutions using a variety of information criteria. library(fpc) set.seed(1) usarr.kcl.3 &lt;- kmeans(USArrests, centers=3, nstart=100) usarr.kcl.5 &lt;- kmeans(USArrests, centers=5, nstart=100) d &lt;- dist(USArrests) cl.stats &lt;- cluster.stats(d, usarr.kcl.3$cluster, usarr.kcl.5$cluster) The corrected Rand index provides a measure for assessing the similarity between two partitions, adjusted for chance. Its range is \\(-1\\) (no agreement) to \\(1\\) (perfect agreement). Using the Rand index, agreement between the cluster solutions is round(cl.stats$corrected.rand,digits=2) ## [1] 0.52 Use a silhouette plot to assess the results when using the kmeans function with different values for \\(K\\). Suggest which value of \\(K\\) you might select to achieve the optimal clustering performance. Reminder: The silhouette width and plot were introduced in Week 8. Solution library(cluster) usarrests.kmeans.2 &lt;- kmeans(USArrests, centers=2, iter.max=1000) usarrests.kmeans.3 &lt;- kmeans(USArrests, centers=3, iter.max=1000) si.k2 &lt;- silhouette(usarrests.kmeans.2$cluster, dist(USArrests)) si.k3 &lt;- silhouette(usarrests.kmeans.3$cluster, dist(USArrests)) par(mfrow=c(1,2)) plot(si.k2, col = c(&quot;red&quot;, &quot;green&quot;), main = &quot;silhouette plot when k=2&quot;) plot(si.k3, col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), main = &quot;silhouette plot when k=3&quot;) We can see that the 2 cluster solution has a higher average silhouette width than the 3 cluster solution. So if we were using this measure to decide, we would select the 2 cluster solution. To test over a larger range of \\(K\\), we can use the function fviz_nbclust from the package factoextra. The argument method allows for different criteria in selecting the optimal number of clusters. library(factoextra) ggplot_fviz_sil &lt;-fviz_nbclust(USArrests, FUN=kmeans, method = &quot;silhouette&quot;) ggplot_fviz_sil #average siluhouette; the higher the better Use the within-cluster sum of squares as the measure to select the optimal value of \\(K\\). Hint Create a plot with the number of clusters on the horizontal axis and the total within cluster sum of squares value for the model with each number of clusters on the vertical axis. The number of \\(K\\) is usually chosen to be where there is a bend in the lines joining the values. Solution set.seed(1) K_max &lt;- 15 #maximum number of clusters wss &lt;- rep(NA, K_max) for (i in 2:K_max){ usarrests.kmeans &lt;- kmeans(USArrests, centers=i, nstart=100) wss[i] &lt;- usarrests.kmeans$tot.withinss } wss[1] &lt;- usarrests.kmeans$totss plot(1:K_max, wss, type=&quot;b&quot;, pch=20, cex=1.5, cex.main=0.9, xlab=&quot;Number of clusters&quot;, ylab=&quot;Within-cluster sum of squares&quot;, main=&quot;Assessing the Optimal Number of Clusters with the Elbow Method&quot;) In this plot, it is slightly hard to find a clear cut-off point. ggplot_fviz_wss &lt;-fviz_nbclust(USArrests, FUN=kmeans, method = &quot;wss&quot;) ggplot_fviz_wss #total within sum of square; same as the Elbow method, look at the knee "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
